
/*
 * Generate Hyperlambda code
 * 
 * This function allows you to generate Hyperlambda code. The [query] argument must be the description of what code you want.
 */
.arguments

   // Mandatory argument describing the Hyperlambda code you want to generate.
   query:string

   // Optional argument being code you want the AI function to change.
   old_code:string

.type:public

try

   // Checking if we've got previously generated code
   if
      and
         exists:x:@.arguments/*/old_code
         not-null:x:@.arguments/*/old_code
      .lambda

         // Making sure we pass in previous code to LLM.
         unwrap:x:+/*/*
         add:x:@try/*/execute/[0,1]/*/arguments
            .
               context:x:@.arguments/*/old_code
               context2:Modify this Hyperlambda code according to my instructions
               
   // Loading system instruction.
   io.file.load:/modules/hyperlambda-generator/system-instruction.md

   /*
    * Invokes OpenAI's chat API with the specified [model], [max_tokens], [context],
    * [instruction] and [query].
    *
    * Will use the default API key found from configurations.
    *
    * [context] is additional data you can supply to OpenAI such that it responds using
    * your data as its source for information. [instruction] is a system message allowing
    * you to change how OpenAI responds.
    */
   execute:magic.workflows.actions.execute
      name:openai-query
      filename:/modules/openai/workflows/actions/openai-query.hl
      arguments
         model:"ft:gpt-4.1-mini-2025-04-14:ainiro:hl:BmxiFN5s"
         max_tokens:int:2500
         temperature:decimal:0.0
         query:x:@.arguments/*/query
         instruction:x:@io.file.load

   // Storing prompt/completion into our database.
   data.connect:hyperlambda-generator
      data.create
         table:requests
         values
            prompt:x:@.arguments/*/query
            completion:x:@execute/*/answer

   /*
    * Buffer for code returned by LLM.
    */
   .new_code
   hyper2lambda:x:@execute/*/answer
      comments:true

   /*
    * Verifying that LLM did NOT return comment, since if it did, we use it as is.
    */
   if
      neq
         get-name:x:@hyper2lambda/0
         .:..
      .lambda

         /*
          * Checking if user provided old code.
          */
         if
            and
               exists:x:@.arguments/*/old_code
               neq:x:@.arguments/*/old_code
                  .:
            .lambda

               /*
                * Checking if old code contained a file level comment, at which point we use it.
                */
               hyper2lambda:x:@.arguments/*/old_code
                  comments:true
               if
                  eq
                     get-name:x:@hyper2lambda/0
                     .:..
                  .lambda

                     // Using old file level comment as comment for newly generated code.
                     unwrap:x:+/*/*
                     insert-before:x:@hyper2lambda/0
                        .
                           ..:x:@hyper2lambda/0
         else

            /*
             * Using prompt as file level comment.
             */
            unwrap:x:+/*/*
            insert-before:x:@hyper2lambda/0
               .
                  ..:x:@.arguments/*/query

   set-value:x:@.new_code
      lambda2hyper:x:@hyper2lambda/*
         comments:true

   // Returns the result of your last action.
   yield
      new_code:x:@.new_code

.catch
   lambda2hyper:x:../*
   log.error:x:@.arguments/*/message
      stack:x:@lambda2hyper
   throw:x:@.arguments/*/message
      status:int:400
      public:bool:true
